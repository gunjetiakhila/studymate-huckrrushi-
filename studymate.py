# -*- coding: utf-8 -*-
"""studymate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fMpugjXyoNu9GCfRScjWpG938BpuL3ba
"""

# -----------------------------
# StudyMate — Single-cell Colab
# PDF Q&A + PyMuPDF chunking + FAISS + Granite 3.3 2B (Hugging Face) + Gradio UI
# -----------------------------

# 1) Install required packages
!pip install -q --upgrade pip
!pip install -q transformers accelerate sentence-transformers faiss-cpu pymupdf gradio huggingface-hub torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118

# 2) Imports
import os, io, time, math, textwrap
from typing import List, Tuple, Dict
import fitz  # PyMuPDF
import numpy as np
import faiss
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from huggingface_hub import login as hf_login
import gradio as gr

# 3) Hugging Face authentication (prompt if not set)
hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN", "").strip()
if not hf_token:
    print("Please paste your Hugging Face token (you can create one at https://huggingface.co/settings/tokens).")
    hf_token = input("Hugging Face token: ").strip()
    if hf_token:
        os.environ["HUGGINGFACEHUB_API_TOKEN"] = hf_token

if not hf_token:
    raise RuntimeError("Hugging Face token required. Set HUGGINGFACEHUB_API_TOKEN env var or paste it when prompted.")

hf_login(token=hf_token)

# 4) Device setup
if torch.cuda.is_available():
    device = "cuda"
    dtype = torch.float16
else:
    device = "cpu"
    dtype = torch.float32
print(f"Device: {device}")

# 5) Load Granite model + tokenizer from Hugging Face
MODEL_ID = "ibm-granite/granite-3.3-2b-instruct"

print("Loading model and tokenizer (this may take a minute)...")
# Try to load on GPU with device_map auto; fallback to CPU if it fails
try:
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype, device_map="auto", trust_remote_code=True, low_cpu_mem_usage=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
except Exception as e:
    print("GPU load failed or not available — loading to CPU (slower). Error:", e)
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True, low_cpu_mem_usage=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

# ensure tokenizer has pad token
if getattr(tokenizer, "pad_token_id", None) is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model.eval()
print("Model loaded.")

# 6) Embedding model (sentence-transformers)
print("Loading sentence-transformers embeddings (all-MiniLM-L6-v2)...")
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# 7) Utilities: PDF extraction & chunking
def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    """Extract selectable text from PDF bytes using PyMuPDF."""
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = []
    for p in doc:
        pages.append(p.get_text("text"))
    doc.close()
    return "\n\n".join(pages)

def chunk_text(text: str, chunk_chars: int = 1500, overlap: int = 200) -> List[Tuple[str,int,int]]:
    """Character-based chunker returning list of (chunk_text, start, end)."""
    text = text.replace("\r\n", "\n")
    n = len(text)
    start = 0
    chunks = []
    while start < n:
        end = min(n, start + chunk_chars)
        # attempt to expand to nearest newline/space to avoid chopping words
        if end < n:
            tail = text[end:end+200]
            nl = tail.find("\n")
            sp = tail.find(" ")
            if nl != -1:
                end = end + nl
            elif sp != -1:
                end = end + sp
        chunk = text[start:end].strip()
        if chunk:
            chunks.append((chunk, start, end))
        start = max(end - overlap, end)  # ensure progress; overlap only if end>overlap
    return chunks

# 8) Retriever with FAISS
class Retriever:
    def __init__(self, embed_model: SentenceTransformer):
        self.embed_model = embed_model
        self.ndim = embed_model.get_sentence_embedding_dimension()
        self.index = None
        self.metadatas = []

    def build(self, texts: List[str], metadatas: List[Dict]):
        embs = self.embed_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
        # normalize for cosine similarity
        faiss.normalize_L2(embs)
        self.index = faiss.IndexFlatIP(self.ndim)
        self.index.add(embs)
        self.metadatas = metadatas

    def query(self, q: str, top_k: int = 5):
        q_emb = self.embed_model.encode([q], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, top_k)
        results = []
        for idx, score in zip(I[0], D[0]):
            if idx < 0 or idx >= len(self.metadatas):
                continue
            md = self.metadatas[idx]
            results.append({"score": float(score), "text": md["text"], "meta": md})
        return results

# 9) RAG prompt & generation helpers
SYSTEM_PROMPT = (
    "You are StudyMate, an academic assistant. Use only the information in the provided document chunks to answer the user's question. "
    "Indicate which chunk(s) you used by including [chunk_i] markers in the answer (i is the chunk index starting at 0). "
    "If the answer isn't contained in the documents, say you don't know and provide general guidance."
)

def build_rag_prompt(question: str, retrieved: List[Dict]) -> str:
    parts = [SYSTEM_PROMPT, "\n\nRetrieved document chunks:\n"]
    for i, r in enumerate(retrieved):
        parts.append(f"[chunk_{i}] (score={r['score']:.3f})\n{r['text']}\n")
    parts.append("\nUser question:\n" + question + "\n\nAnswer:")
    return "\n".join(parts)

def generate_from_model(prompt: str, max_new_tokens: int = 512, temperature: float = 0.0):
    # Tokenize/truncate input according to tokenizer/model limits
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=tokenizer.model_max_length).to(next(model.parameters()).device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=(temperature>0), temperature=temperature, pad_token_id=tokenizer.pad_token_id)
    text = tokenizer.decode(out[0], skip_special_tokens=True)
    # return generated part (best-effort)
    if prompt in text:
        return text.split(prompt, 1)[-1].strip()
    return text.strip()

# 10) State holder
STATE = {"retriever": None, "chunks": [], "metadatas": []}

# 11) Gradio callbacks
def load_pdfs(files):
    all_texts = []
    all_metas = []
    total_chunks = 0
    total_chars = 0
    if not files:
        return "No files uploaded."
    for f in files:
        if f is None:
            continue
        # read bytes
        if hasattr(f, "read"):
            pdf_bytes = f.read()
        else:
            # sometimes Gradio file is a dict with 'name' and 'data'
            with open(f, "rb") as fh:
                pdf_bytes = fh.read()
        text = extract_text_from_pdf_bytes(pdf_bytes)
        if not text.strip():
            continue
        chunks = chunk_text(text, chunk_chars=1500, overlap=200)
        for chunk_text, start, end in chunks:
            meta = {"doc_name": getattr(f, "name", f"uploaded_doc"), "start": start, "end": end, "text": chunk_text}
            all_texts.append(chunk_text)
            all_metas.append(meta)
            total_chars += len(chunk_text)
        total_chunks += len(chunks)

    if not all_texts:
        return "No selectable text found in PDFs. If PDFs are scanned images, OCR is required (not included)."

    retriever = Retriever(embed_model)
    retriever.build(all_texts, all_metas)

    STATE["retriever"] = retriever
    STATE["chunks"] = all_texts
    STATE["metadatas"] = all_metas

    return f"Loaded {len(files)} file(s), {total_chunks} chunks (~{total_chars} characters). Ready for Q&A."

ldef ask_question(question: str, top_k: int = 5):
    if not STATE.get("retriever"):
        return "Please upload and load PDFs first."
    retrieved = STATE["retriever"].query(question, top_k=top_k)
    if not retrieved:
        return "No relevant chunks found."

    prompt = build_rag_prompt(question, retrieved)
    answer = generate_from_model(prompt, max_new_tokens=512, temperature=0.0)

    prov = "\n\n---\nProvenance:\n"
    for i, r in enumerate(retrieved):
        prov += f"[chunk_{i}] doc={r['meta']['doc_name']} chars=({r['meta']['start']}-{r['meta']['end']}) score={r['score']:.3f}\n"

    return answer + prov

# 12) Gradio UI (single-cell)
with gr.Blocks(title="StudyMate — PDF Conversational Q&A (Granite 3.3 2B via Hugging Face)") as demo:
    gr.Markdown("## StudyMate — upload PDFs and ask questions\nUsing **ibm-granite/granite-3.3-2b-instruct** (Hugging Face), **PyMuPDF** for extraction, **FAISS** + **SentenceTransformers** for retrieval.")
    with gr.Row():
        pdf_files = gr.File(file_count="multiple", label="Upload PDF(s)")
        btn_load = gr.Button("Load & Build Index")
    status = gr.Textbox(label="Status", interactive=False, value="No PDFs loaded.")
    with gr.Row():
        qbox = gr.Textbox(lines=2, placeholder="Ask a question about uploaded documents...", label="Question")
        btn_ask = gr.Button("Ask")
    topk = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="Top K retrieved chunks")
    output = gr.Markdown(label="Answer (with provenance)")

    btn_load.click(fn=load_pdfs, inputs=[pdf_files], outputs=[status])
    btn_ask.click(fn=ask_question, inputs=[qbox, topk], outputs=[output])

print("Launching Gradio app... (set share=True in demo.launch if you want a public link)")
demo.launch(share=False, inline=True)

# -----------------------------
# StudyMate — Single-cell Colab
# PDF Q&A + PyMuPDF chunking + FAISS + Granite 3.3 2B (Hugging Face) + Gradio UI
# -----------------------------

# 1) Install required packages
!pip install -q --upgrade pip
!pip install -q transformers accelerate sentence-transformers faiss-cpu pymupdf gradio huggingface-hub torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118

# 2) Imports
import os, io, time, math, textwrap
from typing import List, Tuple, Dict
import fitz  # PyMuPDF
import numpy as np
import faiss
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
from huggingface_hub import login as hf_login
import gradio as gr

# 3) Hugging Face authentication (prompt if not set)
hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN", "").strip()
if not hf_token:
    print("Please paste your Hugging Face token (you can create one at https://huggingface.co/settings/tokens).")
    hf_token = input("Hugging Face token: ").strip()
    if hf_token:
        os.environ["HUGGINGFACEHUB_API_TOKEN"] = hf_token

if not hf_token:
    raise RuntimeError("Hugging Face token required. Set HUGGINGFACEHUB_API_TOKEN env var or paste it when prompted.")

hf_login(token=hf_token)

# 4) Device setup
if torch.cuda.is_available():
    device = "cuda"
    dtype = torch.float16
else:
    device = "cpu"
    dtype = torch.float32
print(f"Device: {device}")

# 5) Load Granite model + tokenizer from Hugging Face
MODEL_ID = "ibm-granite/granite-3.3-2b-instruct"

print("Loading model and tokenizer (this may take a minute)...")
# Try to load on GPU with device_map auto; fallback to CPU if it fails
try:
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=dtype, device_map="auto", trust_remote_code=True, low_cpu_mem_usage=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
except Exception as e:
    print("GPU load failed or not available — loading to CPU (slower). Error:", e)
    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True, low_cpu_mem_usage=True)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

# ensure tokenizer has pad token
if getattr(tokenizer, "pad_token_id", None) is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

model.eval()
print("Model loaded.")

# 6) Embedding model (sentence-transformers)
print("Loading sentence-transformers embeddings (all-MiniLM-L6-v2)...")
embed_model = SentenceTransformer("all-MiniLM-L6-v2")

# 7) Utilities: PDF extraction & chunking
def extract_text_from_pdf_bytes(pdf_bytes: bytes) -> str:
    """Extract selectable text from PDF bytes using PyMuPDF."""
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = []
    for p in doc:
        pages.append(p.get_text("text"))
    doc.close()
    return "\n\n".join(pages)

def chunk_text(text: str, chunk_chars: int = 1500, overlap: int = 200) -> List[Tuple[str,int,int]]:
    """Character-based chunker returning list of (chunk_text, start, end)."""
    text = text.replace("\r\n", "\n")
    n = len(text)
    start = 0
    chunks = []
    while start < n:
        end = min(n, start + chunk_chars)
        # attempt to expand to nearest newline/space to avoid chopping words
        if end < n:
            tail = text[end:end+200]
            nl = tail.find("\n")
            sp = tail.find(" ")
            if nl != -1:
                end = end + nl
            elif sp != -1:
                end = end + sp
        chunk = text[start:end].strip()
        if chunk:
            chunks.append((chunk, start, end))
        start = max(end - overlap, end)  # ensure progress; overlap only if end>overlap
    return chunks

# 8) Retriever with FAISS
class Retriever:
    def __init__(self, embed_model: SentenceTransformer):
        self.embed_model = embed_model
        self.ndim = embed_model.get_sentence_embedding_dimension()
        self.index = None
        self.metadatas = []

    def build(self, texts: List[str], metadatas: List[Dict]):
        embs = self.embed_model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
        # normalize for cosine similarity
        faiss.normalize_L2(embs)
        self.index = faiss.IndexFlatIP(self.ndim)
        self.index.add(embs)
        self.metadatas = metadatas

    def query(self, q: str, top_k: int = 5):
        q_emb = self.embed_model.encode([q], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, top_k)
        results = []
        for idx, score in zip(I[0], D[0]):
            if idx < 0 or idx >= len(self.metadatas):
                continue
            md = self.metadatas[idx]
            results.append({"score": float(score), "text": md["text"], "meta": md})
        return results

# 9) RAG prompt & generation helpers
SYSTEM_PROMPT = (
    "You are StudyMate, an academic assistant. Use only the information in the provided document chunks to answer the user's question. "
    "Indicate which chunk(s) you used by including [chunk_i] markers in the answer (i is the chunk index starting at 0). "
    "If the answer isn't contained in the documents, say you don't know and provide general guidance."
)

def build_rag_prompt(question: str, retrieved: List[Dict]) -> str:
    parts = [SYSTEM_PROMPT, "\n\nRetrieved document chunks:\n"]
    for i, r in enumerate(retrieved):
        parts.append(f"[chunk_{i}] (score={r['score']:.3f})\n{r['text']}\n")
    parts.append("\nUser question:\n" + question + "\n\nAnswer:")
    return "\n".join(parts)

def generate_from_model(prompt: str, max_new_tokens: int = 512, temperature: float = 0.0):
    # Tokenize/truncate input according to tokenizer/model limits
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=tokenizer.model_max_length).to(next(model.parameters()).device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=(temperature>0), temperature=temperature, pad_token_id=tokenizer.pad_token_id)
    text = tokenizer.decode(out[0], skip_special_tokens=True)
    # return generated part (best-effort)
    if prompt in text:
        return text.split(prompt, 1)[-1].strip()
    return text.strip()

# 10) State holder
STATE = {"retriever": None, "chunks": [], "metadatas": []}

# 11) Gradio callbacks
def load_pdfs(files):
    all_texts = []
    all_metas = []
    total_chunks = 0
    total_chars = 0
    if not files:
        return "No files uploaded."
    for f in files:
        if f is None:
            continue
        # read bytes
        if hasattr(f, "read"):
            pdf_bytes = f.read()
        else:
            # sometimes Gradio file is a dict with 'name' and 'data'
            with open(f, "rb") as fh:
                pdf_bytes = fh.read()
        text = extract_text_from_pdf_bytes(pdf_bytes)
        if not text.strip():
            continue
        chunks = chunk_text(text, chunk_chars=1500, overlap=200)
        for chunk_text, start, end in chunks:
            meta = {"doc_name": getattr(f, "name", f"uploaded_doc"), "start": start, "end": end, "text": chunk_text}
            all_texts.append(chunk_text)
            all_metas.append(meta)
            total_chars += len(chunk_text)
        total_chunks += len(chunks)

    if not all_texts:
        return "No selectable text found in PDFs. If PDFs are scanned images, OCR is required (not included)."

    retriever = Retriever(embed_model)
    retriever.build(all_texts, all_metas)

    STATE["retriever"] = retriever
    STATE["chunks"] = all_texts
    STATE["metadatas"] = all_metas

    return f"Loaded {len(files)} file(s), {total_chunks} chunks (~{total_chars} characters). Ready for Q&A."

def ask_question(question: str, top_k: int = 5):
    if not STATE.get("retriever"):
        return "Please upload and load PDFs first."
    retrieved = STATE["retriever"].query(question, top_k=top_k)
    if not retrieved:
        return "No relevant chunks found."

    prompt = build_rag_prompt(question, retrieved)
    answer = generate_from_model(prompt, max_new_tokens=512, temperature=0.0)

    prov = "\n\n---\nProvenance:\n"
    for i, r in enumerate(retrieved):
        prov += f"[chunk_{i}] doc={r['meta']['doc_name']} chars=({r['meta']['start']}-{r['meta']['end']}) score={r['score']:.3f}\n"

    return answer + prov

# 12) Gradio UI (single-cell)
with gr.Blocks(title="StudyMate — PDF Conversational Q&A (Granite 3.3 2B via Hugging Face)") as demo:
    gr.Markdown("## StudyMate — upload PDFs and ask questions\nUsing **ibm-granite/granite-3.3-2b-instruct** (Hugging Face), **PyMuPDF** for extraction, **FAISS** + **SentenceTransformers** for retrieval.")
    with gr.Row():
        pdf_files = gr.File(file_count="multiple", label="Upload PDF(s)")
        btn_load = gr.Button("Load & Build Index")
    status = gr.Textbox(label="Status", interactive=False, value="No PDFs loaded.")
    with gr.Row():
        qbox = gr.Textbox(lines=2, placeholder="Ask a question about uploaded documents...", label="Question")
        btn_ask = gr.Button("Ask")
    topk = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="Top K retrieved chunks")
    output = gr.Markdown(label="Answer (with provenance)")

    btn_load.click(fn=load_pdfs, inputs=[pdf_files], outputs=[status])
    btn_ask.click(fn=ask_question, inputs=[qbox, topk], outputs=[output])

print("Launching Gradio app... (set share=True in demo.launch if you want a public link)")
demo.launch(share=False, inline=True)